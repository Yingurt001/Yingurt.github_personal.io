<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Computer Vision Lesson 2: CNN Fundamentals - Ying Zhang</title>
    <link rel="stylesheet" href="../styles.css">
    <!-- MathJax for rendering mathematical equations -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            }
        };
    </script>
</head>
<body>
    <!-- Navigation Bar -->
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-links">
                <a href="../index.html">
                    <svg fill="currentColor" viewBox="0 0 20 20"><path d="M10.707 2.293a1 1 0 00-1.414 0l-7 7a1 1 0 001.414 1.414L4 10.414V17a1 1 0 001 1h2a1 1 0 001-1v-2a1 1 0 011-1h2a1 1 0 011 1v2a1 1 0 001 1h2a1 1 0 001-1v-6.586l.293.293a1 1 0 001.414-1.414l-7-7z"></path></svg>
                    Home
                </a>
                <a href="../blog.html">
                    <svg fill="currentColor" viewBox="0 0 20 20"><path d="M2 5a2 2 0 012-2h7a2 2 0 012 2v4a2 2 0 01-2 2H9l-3 3v-3H4a2 2 0 01-2-2V5z"></path><path d="M15 7v2a4 4 0 01-4 4H9.828l-1.766 1.767c.28.149.599.233.938.233h2l3 3v-3h2a2 2 0 002-2V9a2 2 0 00-2-2h-1z"></path></svg>
                    Blog
                </a>
            </div>
        </div>
    </nav>

    <div class="container blog-full-width">
        <main class="content blog-post">
            <a href="../blog.html" class="back-to-blog">‚Üê Back to Blog</a>
            
            <article>
                <div class="blog-post-header">
                    <h1 class="blog-post-title">Computer Vision Lesson 2: Understanding the Convolutional Base</h1>
                    <div class="blog-post-meta">
                        <span>üìÖ November 24, 2025</span>
                        <span>üè∑Ô∏è Computer Vision, CNN, Convolution, ReLU, TensorFlow, Deep Learning</span>
                    </div>
                    <div style="margin-top: 1rem;">
                        <a href="https://github.com/Yingurt001/Computer-Vision" target="_blank" style="color: #3b82f6; text-decoration: none; font-weight: 500;">
                            üîó View Repository on GitHub ‚Üí
                        </a>
                    </div>
                </div>

                <div class="blog-post-content">
                    <h2>Introduction</h2>
                    <p>
                        In Lesson 1, we learned that our convolutional classifier has two parts:
                    </p>
                    <ol>
                        <li><strong>A convolutional base</strong> - extracts visual features from an image</li>
                        <li><strong>A head of dense layers</strong> - uses those features to classify the image</li>
                    </ol>
                    <p>
                        In this lesson, we focus on understanding what the convolutional base is doing during CNN. We don't need to go into details about both parts, but we will try our best to understand the convolutional base.
                    </p>

                    <h2>What Do We Learn in This Lesson?</h2>
                    <p>
                        We only learn one thing:
                    </p>
                    <ul>
                        <li><strong>Convolutional layer with ReLU activation</strong></li>
                    </ul>
                    <div style="background-color: #fef3c7; border-left: 4px solid #f59e0b; padding: 1rem; margin: 1.5rem 0; border-radius: 4px;">
                        <p style="margin: 0;"><strong>‚ö†Ô∏è Important Note:</strong> A convolutional layer is <em>not</em> equivalent to a filter. Our filter often does not include activation, but is just a kernel scanning that outputs the value for each grid cell.</p>
                    </div>

                    <h2>Feature Extraction</h2>
                    <p>
                        We first need to discuss the purpose of these layers in the network. Feature extraction consists of three basic operations:
                    </p>
                    <ol>
                        <li><strong>Filter</strong> an image for a particular feature (convolution)</li>
                        <li><strong>Detect</strong> that feature within the filtered image (ReLU)</li>
                        <li><strong>Condense</strong> the image to enhance the features (maximum pooling)</li>
                    </ol>

                    <p>
                        Below is the figure for the Feature extraction of 4 stages: Input, Filter, Detect, and Condense.
                    </p>
                    <div style="text-align: center; margin: 2rem 0;">
                        <img src="../blog-images/Feature_extraction.png" alt="Feature Extraction Process" style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
                    </div>

                    <h2>First Layer: Filter with Convolution</h2>
                    <p>
                        A convolutional layer carries out the filtering step.
                    </p>

                    <h3>Weights and Kernels</h3>
                    <p>
                        The first figure shows what we call <strong>weights</strong>, which is what the convnet learns during training. These are primarily contained in its convolutional layers. These weights we call <strong>kernels</strong>. We can represent them as small arrays.
                    </p>
                    <p>
                        This figure shows a <strong>3√ó3 convolution kernel</strong> (filter). The weight values are:
                    </p>
                    <pre style="background-color: #1f2937; color: #f9fafb; padding: 1rem; border-radius: 6px; overflow-x: auto;"><code>[-1   2   -1]
[-1   2   -1]
[-1   2   -1]</code></pre>

                    <div style="text-align: center; margin: 2rem 0;">
                        <img src="../blog-images/weights.png" alt="Convolution Kernel Weights" style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
                    </div>

                    <h3>The Convolution Operation</h3>
                    <p>
                        The convolution operation does:
                    </p>
                    <div style="text-align: center; margin: 1.5rem 0;">
                        <code style="background-color: #f3f4f6; padding: 0.5rem 1rem; border-radius: 4px; font-size: 1.1rem;">output = sum(kernel[i,j] * input_patch[i,j])</code>
                    </div>

                    <p>
                        Figure 2 shows <strong>The Convolution Extraction Process</strong>. The kernel slides across the image.
                    </p>

                    <div style="text-align: center; margin: 2rem 0;">
                        <img src="../blog-images/kernel.png" alt="Convolution Extraction Process" style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
                    </div>

                    <h3>Understanding the Process</h3>
                    <ul>
                        <li><strong>Weights plot</strong> shows <em>what</em> the kernel looks like (the weights).</li>
                        <li><strong>Convolution Extraction Process</strong> shows <em>how</em> that kernel is used to create a feature map.</li>
                    </ul>

                    <p>
                        So together they illustrate:
                    </p>
                    <ol>
                        <li>A convolution kernel is a small grid of weights</li>
                        <li>It scans across the image</li>
                        <li>At each location it produces a single output value</li>
                        <li>These values assemble into a feature map</li>
                        <li>The feature map highlights the features the kernel is designed to detect (e.g., vertical edges)</li>
                    </ol>

                    <p>
                        A kernel operates by scanning over an image and producing a weighted sum of pixel values. In this way, a kernel will act sort of like a polarized lens, emphasizing or deemphasizing certain patterns of information.
                    </p>

                    <h3>Implementing Convolution in TensorFlow</h3>
                    <pre style="background-color: #1f2937; color: #f9fafb; padding: 1rem; border-radius: 6px; overflow-x: auto;"><code>from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Conv2D(filters=64, kernel_size=3), # activation is None
    # More layers follow
])

'''
keras.Sequential means that I am building a model by stacking layers one after another

layers gives me the access to Keras layer types (Conv2D)

Conv2D is a 2D convolution layer used for images
'''</code></pre>

                    <h3>Understanding the Parameters</h3>
                    <p>
                        <strong>Filter = 64</strong> means that the layer will learn 64 different feature maps.
                    </p>
                    <p>
                        If I set <code>filters=64</code>, then:
                    </p>
                    <ul>
                        <li>I have 64 different kernels</li>
                        <li>Each kernel learns a different pattern</li>
                        <li>Each kernel shares weights across space</li>
                    </ul>

                    <p>
                        Because some features like edges, textures, or corners can appear anywhere in the image, they should be detected in the same way like other pixels.
                    </p>

                    <p>
                        Normally we would set the size of the kernel to be odd like <code>kernel_size=(3, 3)</code> or <code>(5, 5)</code> so that a single pixel sits at the center.
                    </p>

                    <h3>More Fun Facts About Kernel</h3>
                    <p>
                        Each output neuron is related to nine neurons in the input. By setting the dimensions of the kernels with <code>kernel_size</code>, we will be able to tell the convnet how to form the connections.
                    </p>

                    <h2>Second Layer: Activations</h2>
                    <p>
                        After the convolution layer produces its feature maps, the activation layer (like ReLU) applies a nonlinear function to those maps.
                    </p>

                    <h3>Why Do We Need Activation?</h3>
                    <p>
                        Because without activation, a CNN would be only linear operations, meaning it cannot learn complex patterns. Activation makes the network capable of learning nonlinear features, such as shapes, textures, and objects.
                    </p>

                    <h3>What the Activation Does</h3>
                    <ul>
                        <li>Keeps positive values (ReLU: max(0, x))</li>
                        <li>Suppresses irrelevant negative activations</li>
                        <li><strong>Adds nonlinearity</strong></li>
                    </ul>

                    <h3>How They Work Together</h3>
                    <p>
                        <strong>Convolution layer produces:</strong>
                    </p>
                    <ul>
                        <li>Raw feature maps (linear responses)</li>
                    </ul>
                    <p>
                        <strong>Activation layer produces:</strong>
                    </p>
                    <ul>
                        <li>Activated feature maps</li>
                    </ul>

                    <h3>ReLU Activation</h3>
                    <p>
                        Below is a figure of ReLU activation:
                    </p>

                    <div style="text-align: center; margin: 2rem 0;">
                        <img src="../blog-images/ReLU.png" alt="ReLU Activation Function" style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
                    </div>

                    <p>
                        This means:
                    </p>
                    <ul>
                        <li>If <strong>x < 0</strong>, the output is <strong>0</strong></li>
                        <li>If <strong>x ‚â• 0</strong>, the output is <strong>x</strong></li>
                    </ul>

                    <p>
                        We would also call the rectifier function the <strong>ReLU activation</strong> or even the <strong>ReLU function</strong>. It can be defined in its own Activation layer, but most often we would also include it as the activation function of Conv2D.
                    </p>

                    <p>
                        After a convolution produces a feature map (linear output), ReLU is applied:
                    </p>
                    <div style="text-align: center; margin: 1.5rem 0; padding: 1rem; background-color: #f0f9ff; border-radius: 6px;">
                        <code style="font-size: 1.1rem;">Convolution ‚Üí Feature Map ‚Üí ReLU ‚Üí Activated Feature Map</code>
                    </div>

                    <h3>Why Is This Important?</h3>
                    <p>
                        We could think about the activation function as scoring pixel values according to some measure of importance. Every unimportant value is equally unimportant.
                    </p>

                    <h2>Summary</h2>
                    <div style="background-color: #fef3c7; border-left: 4px solid #f59e0b; padding: 1rem; margin: 1.5rem 0; border-radius: 4px;">
                        <p style="margin: 0; font-size: 1.1rem; font-weight: 600;">
                            <strong>‚≠ê Key Takeaway:</strong> The convolutional base performs feature extraction through three steps: <strong>Filter</strong> (convolution), <strong>Detect</strong> (ReLU activation), and <strong>Condense</strong> (max pooling). Understanding how convolution kernels scan images and how ReLU activation adds nonlinearity is crucial for building effective CNNs.
                        </p>
                    </div>

                    <h2>Learning Outcomes</h2>
                    <p>Through this lesson, I have mastered:</p>
                    <ul>
                        <li>‚úÖ Understanding the structure of convolutional classifiers (base + head)</li>
                        <li>‚úÖ How convolutional layers perform filtering through kernel scanning</li>
                        <li>‚úÖ The relationship between kernels, weights, and feature maps</li>
                        <li>‚úÖ How to use Conv2D layers in TensorFlow/Keras</li>
                        <li>‚úÖ The importance of ReLU activation for adding nonlinearity</li>
                        <li>‚úÖ How convolution and activation work together in feature extraction</li>
                    </ul>

                    <h2>Related Files</h2>
                    <p>
                        The complete code implementation can be found in the repository:
                    </p>
                    <ul>
                        <li><a href="https://github.com/Yingurt001/Computer-Vision/blob/main/lecture2.ipynb" target="_blank">lecture2.ipynb</a> - Complete code implementation</li>
                        <li><a href="https://github.com/Yingurt001/Computer-Vision/blob/main/Lesson_2.md" target="_blank">Lesson_2.md</a> - Lesson notes</li>
                        <li><a href="https://github.com/Yingurt001/Computer-Vision/blob/main/CNN_Lesson_2.md" target="_blank">CNN_Lesson_2.md</a> - Detailed lesson summary</li>
                    </ul>

                    <blockquote style="background-color: #f0f9ff; border-left: 4px solid #0ea5e9; padding: 1rem; margin: 1.5rem 0; border-radius: 4px;">
                        <strong>üí° Key Insight:</strong> The convolutional base is the feature extraction engine of CNNs. By understanding how kernels scan images and how ReLU activation introduces nonlinearity, we can better design and optimize convolutional neural networks for various computer vision tasks.
                    </blockquote>

                    <div style="margin-top: 2rem; padding: 1rem; background-color: #f9fafb; border-radius: 6px;">
                        <p style="margin: 0;"><strong>Repository:</strong> <a href="https://github.com/Yingurt001/Computer-Vision" target="_blank" style="color: #3b82f6;">https://github.com/Yingurt001/Computer-Vision</a></p>
                    </div>
                </div>
            </article>
        </main>
    </div>

    <footer class="footer">
        <div class="footer-content">
            <div class="footer-links">
                <a href="https://github.com/Yingurt001" target="_blank" title="GitHub"><img src="https://img.icons8.com/color/24/000000/github.png" alt="GitHub"></a>
                <a href="https://orcid.org/0009-0000-2900-9197" target="_blank" title="ORCID"><img src="https://upload.wikimedia.org/wikipedia/commons/archive/f/f7/20160723044737%21Orcid_icon.png" alt="ORCID"></a>
            </div>
            <div class="footer-copyright">¬© Ying Zhang</div>
        </div>
    </footer>
</body>
</html>
