<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Understanding Transformer: Attention Is All You Need - Ying Zhang</title>
    <link rel="stylesheet" href="../styles.css">
    <!-- MathJax for rendering mathematical equations -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            }
        };
    </script>
</head>
<body>
    <!-- Navigation Bar -->
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-links">
                <a href="../index.html">
                    <svg fill="currentColor" viewBox="0 0 20 20"><path d="M10.707 2.293a1 1 0 00-1.414 0l-7 7a1 1 0 001.414 1.414L4 10.414V17a1 1 0 001 1h2a1 1 0 001-1v-2a1 1 0 011-1h2a1 1 0 011 1v2a1 1 0 001 1h2a1 1 0 001-1v-6.586l.293.293a1 1 0 001.414-1.414l-7-7z"></path></svg>
                    Home
                </a>
                <a href="../blog.html">
                    <svg fill="currentColor" viewBox="0 0 20 20"><path d="M2 5a2 2 0 012-2h7a2 2 0 012 2v4a2 2 0 01-2 2H9l-3 3v-3H4a2 2 0 01-2-2V5z"></path><path d="M15 7v2a4 4 0 01-4 4H9.828l-1.766 1.767c.28.149.599.233.938.233h2l3 3v-3h2a2 2 0 002-2V9a2 2 0 00-2-2h-1z"></path></svg>
                    Blog
                </a>
            </div>
        </div>
    </nav>

    <div class="container blog-full-width">
        <main class="content blog-post">
            <a href="../blog.html" class="back-to-blog">‚Üê Back to Blog</a>
            
            <article>
                <div class="blog-post-header">
                    <h1 class="blog-post-title">Understanding Transformer: Attention Is All You Need</h1>
                    <div class="blog-post-meta">
                        <span>üìÖ November 24, 2025</span>
                        <span>üè∑Ô∏è Machine Learning, Deep Learning, Transformer, Attention Mechanism, NLP</span>
                    </div>
                    <div style="margin-top: 1rem;">
                        <p style="color: #666; font-size: 0.9rem;">
                            Inspired by <a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" style="color: #3b82f6;">The Illustrated Transformer</a>
                        </p>
                    </div>
                </div>

                <div class="blog-post-content">
                    <h2>Introduction</h2>
                    <p>
                        Transformer was first proposed in the paper <strong>"Attention is all you need"</strong>. Today, after reading the interpretation of Transformer on <a href="https://jalammar.github.io/illustrated-transformer/" target="_blank">The Illustrated Transformer</a>, I decided to write a reading reflection to analyze the remarkable aspects of Transformer.
                    </p>
                    <p>
                        Let's start from a high-level perspective. We begin with a specific domain‚Äîmachine translation. We input a Chinese sentence and output the corresponding English translation.
                    </p>
                    <p>
                        From an external perspective without seeing the internal structure of Transformer, we see that our input enters <strong>Encoders</strong>, and then passes through <strong>Decoders</strong> to get the final result.
                    </p>

                    <div style="text-align: center; margin: 2rem 0;">
                        <img src="../blog-images/Trans_1.png" alt="Transformer Architecture Overview" style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
                    </div>

                    <h3>Encoder Structure</h3>
                    <p>
                        The encoding component (Encoder) is not just one, but a series of Encoders stacked together. All encoders have the same structure, but their weights are different. Each encoder is divided into two sub-layers:
                    </p>
                    <ol>
                        <li><strong>Self-Attention Layer</strong> - This layer helps the encoder focus on other words in the input sentence while encoding a specific word. We will discuss the self-attention layer in detail later.</li>
                        <li><strong>Feed Forward Layer</strong> - A fully connected neural network.</li>
                    </ol>

                    <h3>Input Transformation</h3>
                    <p>
                        For example, a sentence "Êàë ÊòØ ‰∏Ä‰∏™ Â≠¶Áîü" (I am a student). We segment it according to our understanding to get these four words.
                    </p>
                    <p>
                        We use an <strong>"Embedding Algorithm"</strong> to transform each word into a vector.
                    </p>

                    <div style="text-align: center; margin: 2rem 0;">
                        <img src="../blog-images/Trans_2.png" alt="Word Embedding" style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
                    </div>

                    <p>
                        This embedding only happens in the first encoder layer. The input to subsequent encoders is directly the already encoded vectors. A common principle for all encoders is that they all accept a list of vectors of size 512.
                    </p>
                    <p>
                        The size of this list is a configurable hyperparameter‚Äîit basically equals the length of the longest sentence in the training dataset.
                    </p>
                    <p>
                        After we embed the words, we have already prepared two layers of our encoding layer‚Äî<strong>Self Attention</strong> and <strong>Feed Forward layer</strong>, as shown below:
                    </p>

                    <div style="text-align: center; margin: 2rem 0;">
                        <img src="../blog-images/Trans_3.png" alt="Encoder Layers" style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
                    </div>

                    <div style="background-color: #fef3c7; border-left: 4px solid #f59e0b; padding: 1rem; margin: 1.5rem 0; border-radius: 4px;">
                        <p style="margin: 0;"><strong>üí° Important Points:</strong></p>
                        <ul style="margin: 0.5rem 0 0 0;">
                            <li>Although X‚ÇÅ, X‚ÇÇ, X‚ÇÉ... appear to enter Self-Attention independently, they are actually related to each other. As each word path flows, they generate associations because "self-attention" means looking at the relationship between this word and other words.</li>
                            <li>When Z‚ÇÅ, Z‚ÇÇ, Z‚ÇÉ... enter Feed-Forward, they are processed independently. Each path can be executed in parallel when flowing through the Feed Forward layer.</li>
                        </ul>
                    </div>

                    <h2>Encoding Process</h2>
                    <p>
                        We just discussed one Encoder. Now let's talk about the second Encoder and how they are connected.
                    </p>
                    <p>
                        Each word at each position goes through a self-attention process, and then they each enter a feed-forward neural network‚Äîthe same network, but each vector flows through it separately.
                    </p>

                    <div style="text-align: center; margin: 2rem 0;">
                        <img src="../blog-images/Trans_4.png" alt="Encoding Process" style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
                    </div>

                    <h2>Self-Attention at a High Level</h2>
                    <p>
                        Consider this sentence: <em>"The animal didn't cross the street because it was too tired"</em>. What does "it" refer to‚Äî"animal" or "street"?
                    </p>
                    <p>
                        Actually, we humans can answer this question well, but for machines, this is somewhat difficult to understand. This is where self-attention comes into play.
                    </p>
                    <p>
                        Transformer's self-attention mechanism allows the model to look at other positions in the input sequence while processing each word, searching for clues that help encode that word. Remember why we said that each word at each position is related in Self-Attention? This is the reason.
                    </p>

                    <h2>Self-Attention in Detail</h2>
                    <p>
                        The first step in calculating self-attention is to create three vectors from each input vector of the encoder (note: already embedded). Therefore, for each word, we create:
                    </p>
                    <ul>
                        <li>A <strong>Query vector Q</strong></li>
                        <li>A <strong>Key vector K</strong></li>
                        <li>A <strong>Value vector V</strong></li>
                    </ul>

                    <p>
                        These vectors are obtained by multiplying the word by our trained matrices during training.
                    </p>
                    <p>
                        <strong>Note:</strong> Here we're talking about single-head attention. Later we'll discuss multi-head attention, which is essentially multiple attentions with weighted summation. But for simplicity, we'll first explain the case with only one attention.
                    </p>
                    <p>
                        The Query vector Q, Key vector K, and Value vector V are typically 64-dimensional, which is much smaller than the embedding vector of 512. This is an architectural choice, not a requirement, because we want to keep the computation of multi-head attention constant.
                    </p>

                    <h3>Calculating Attention</h3>
                    <p>
                        Suppose we want to calculate the self-attention score for a word "Thinking". The score calculation method is to compute the dot product of the query vector with the key vector of the word to be scored. Therefore, if we are processing the self-attention mechanism of the word at position #1, the first score is the dot product of q‚ÇÅ and k‚ÇÅ. The second score is the dot product of q‚ÇÅ and k‚ÇÇ.
                    </p>

                    <div style="text-align: center; margin: 2rem 0;">
                        <img src="../blog-images/Trans_5.png" alt="Self-Attention Calculation" style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
                    </div>

                    <p>
                        The third step is to divide the score value by 8 (the square root of 64, the dimension of the key vector used in the paper). This yields more stable gradients.
                    </p>
                    <p>
                        The fourth step is to feed the result into a softmax operation. The softmax operation normalizes the scores so that they are all positive and sum to one.
                    </p>

                    <div style="background-color: #f0f9ff; border-left: 4px solid #0ea5e9; padding: 1rem; margin: 1.5rem 0; border-radius: 4px;">
                        <p style="margin: 0;"><strong>üí° Why do we do this?</strong></p>
                        <p style="margin: 0.5rem 0 0 0;">
                            Softmax scores determine how often each word appears at that position. Obviously, the word at that position should have the highest softmax score.
                        </p>
                    </div>

                    <p>
                        The fifth step is to multiply each value vector by the softmax score, preparing to sum them. The purpose here is to keep the values of words we want to focus on unchanged and eliminate the influence of irrelevant words (e.g., multiply by small decimals like 0.001).
                    </p>
                    <p>
                        The sixth step is to sum the weighted value vectors, which produces the output of the self-attention layer at that position for the first word. Finally, the z‚ÇÅ we get will represent that this word should contain information from this word itself and other words. It will no longer be a single word, but an aggregation of multiple words.
                    </p>

                    <h2>Matrix Calculation on Self-Attention</h2>
                    <p>
                        Now we're not just calculating one word, but a real sentence input:
                    </p>

                    <div style="text-align: center; margin: 2rem 0;">
                        <img src="../blog-images/Trans_6.png" alt="Matrix Calculation" style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
                    </div>

                    <p>
                        Each row of X represents a word, then multiplied by their same weight matrices W^Q, W^K, W^V for a certain head.
                    </p>
                    <p>
                        Finally, we use our softmax formula:
                    </p>

                    <div style="text-align: center; margin: 2rem 0;">
                        <img src="../blog-images/Trans_7.png" alt="Softmax Formula" style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
                    </div>

                    <h2>The Beast With Many Heads</h2>
                    <p>
                        We now add the "multi-head" attention mechanism to further improve the self-attention layer.
                    </p>

                    <div style="text-align: center; margin: 2rem 0;">
                        <img src="../blog-images/Trans_8.png" alt="Multi-Head Attention" style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
                    </div>

                    <p>
                        Previously, we had one Q, K, V. Now we generate multiple QKV and their corresponding training matrices. For example, if we have eight heads, we'll get eight Z matrices z‚ÇÅ, z‚ÇÇ, z‚ÇÉ, ..., z‚Çà through softmax.
                    </p>
                    <p>
                        These 8 Z matrices will finally get a final Z matrix according to a weight matrix.
                    </p>

                    <div style="text-align: center; margin: 2rem 0;">
                        <img src="../blog-images/Trans_9.png" alt="Multi-Head Concatenation" style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
                    </div>

                    <p>
                        Based on the above, we finally output this diagram:
                    </p>

                    <div style="text-align: center; margin: 2rem 0;">
                        <img src="../blog-images/Trans_10.png" alt="Complete Multi-Head Attention Process" style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
                    </div>

                    <p>
                        This diagram completely shows how Multi-Head Self-Attention in the Transformer encoder processes the input sentence. Starting from the leftmost side, the diagram gives an example sentence "Thinking Machines". When entering the first encoder layer, each word is first converted into a vector through embedding. Therefore, the green matrix X in the diagram represents a set of vectors corresponding to each word in the sentence, with each row being a word's embedding.
                    </p>
                    <p>
                        For encoder layer 0, we need to get X from the original text through embedding; for layer 1, layer 2, and other higher layers, their input is no longer the text embedding, but the output from the layer below, so the embedding operation is only performed once at the bottom layer.
                    </p>
                    <p>
                        Then, the model projects this matrix X into different Query, Key, and Value representations, and does this for "8 heads" simultaneously, not just one set. Therefore, the diagram shows 8 sets of weight matrices W^Q_h, W^K_h, W^V_h. Each set of differently colored matrices represents the weights used by one attention head.
                    </p>
                    <p>
                        By multiplying X with these weight matrices respectively, the model generates three new representations for each head: Query, Key, and Value. The reason for having multiple heads is that Transformer hopes to understand the structure of the same sentence from multiple different "angles" or "attention directions". Each head will be smaller (e.g., dimension reduced from 512 to 64), but because there are 8 heads, they together cover "multiple possible attention directions".
                    </p>
                    <p>
                        When all Query, Key, and Value are generated, the diagram enters the next step: each head independently performs attention calculation. That is, for head 0, we use Q‚ÇÄ and K‚ÇÄ to calculate similarity scores, divide the result by the square root of the vector dimension (e.g., ‚àö64), then feed it into softmax to get weights representing "which words should be paid more attention to". Then, use these weights to weight the Value matrix V‚ÇÄ to get the output Z‚ÇÄ of this head.
                    </p>
                    <p>
                        After all eight heads complete the attention operation, the model concatenates these pink output matrices column by column to form a larger matrix Z. This step can be understood as: each head provides part of the information, and now these eight parts of information are recombined into a whole to form a global understanding of the input sentence.
                    </p>
                    <p>
                        However, the concatenated matrix Z cannot be directly used as the final output of this layer‚Äîbecause the model needs to linearly mix the multi-head results again so that information between different heads can interact or reorganize. Therefore, a large pink weight matrix W^O appears on the right side of the diagram. Z is multiplied with W^O to get the final encoder layer output. The dimension of this output returns to the same dimension as the original embedding, allowing it to continue as input to the next Transformer encoder layer.
                    </p>

                    <h2>Positional Encoding</h2>
                    <p>
                        So far, the model we described still lacks a way to consider the order of words in the input sequence. We only output the importance degree of words at different positions, without considering the sequence order between them.
                    </p>
                    <p>
                        Transformer adds a vector to each input embedding. These vectors follow a specific pattern learned by the model, which helps the model determine the position of each word, or the distance between different words in the sequence. The intuition is that adding these values to the embeddings can provide meaningful distances when the embedding vectors are projected into Q/K/V vectors and when performing dot product attention mechanism.
                    </p>

                    <h2>Residual Connections</h2>
                    <p>
                        Each encoder has a residual connection around each sub-layer (self-attention, FFNN), and then a layer normalization step is performed.
                    </p>

                    <div style="text-align: center; margin: 2rem 0;">
                        <img src="../blog-images/Trans_11.png" alt="Residual Connections" style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
                    </div>

                    <h2>Decoder</h2>
                    <p>
                        The encoder processes the input sequence. The output of the top encoder is transformed into a set of attention vectors K and V, which each decoder uses in its "encoder-decoder attention" to help the decoder focus on appropriate positions in the input sequence.
                    </p>

                    <h3>The Fundamental Role of Decoder</h3>
                    <p>
                        In one sentence:
                    </p>
                    <blockquote style="background-color: #f0f9ff; border-left: 4px solid #0ea5e9; padding: 1rem; margin: 1.5rem 0; border-radius: 4px;">
                        The decoder is responsible for predicting the next word based on the already generated output (e.g., the partially translated sentence) and combined with the full sentence understanding provided by the encoder.
                    </blockquote>

                    <p>
                        In simpler terms:
                    </p>
                    <ul>
                        <li><strong>Encoder</strong> at the input end: Understands the meaning of the entire sentence</li>
                        <li><strong>Decoder</strong> at the output end: Decides the next word to generate based on "what words have I already generated" and "the meaning of the source sentence"</li>
                        <li>The whole process is executed repeatedly: one word at a time until the complete output is generated</li>
                    </ul>

                    <p>
                        Therefore, the decoder is <strong>the core of the language generation process</strong>.
                    </p>
                    <p>
                        Whether you're doing machine translation, text generation, dialogue, summarization, fill-in-the-blank, or reasoning generation like ChatGPT, they are all driven by the "decoder idea" of Transformer.
                    </p>

                    <h3>Three Core Functions of Decoder</h3>
                    <p>
                        The decoder actually undertakes three key tasks internally:
                    </p>

                    <h4>Function 1: Understanding the Generated Partial Output (Self-Attention + Masking)</h4>
                    <p>
                        When the decoder generates a sentence (such as a translation result), it doesn't produce it all at once, but word by word.
                    </p>
                    <p>
                        For example, when translating "Je suis heureux", if the decoder has already generated "I am", it now needs to predict the next word "happy".
                    </p>
                    <p>
                        To do this, it needs to:
                    </p>
                    <ul>
                        <li>Understand the words already generated: I, am</li>
                        <li>Understand the relationships between them</li>
                        <li>But cannot "peek" at future words that haven't been generated yet (e.g., happy)</li>
                    </ul>
                    <p>
                        ‚Üí Because that would violate the autoregressive generation principle.
                    </p>
                    <p>
                        Therefore, <strong>Mask (future positions are masked)</strong> is added to the decoder's self-attention.
                    </p>
                    <p>
                        <strong>Summary:</strong> Decoder self-attention is responsible for letting the model understand "what have I said so far?"
                    </p>

                    <h4>Function 2: Using Encoder's Context Information (Cross-Attention)</h4>
                    <p>
                        Next, the decoder needs to use the full sentence understanding provided by the encoder.
                    </p>
                    <p>
                        The encoder provides a deep representation of the input sentence "Thinking Machines". When the decoder predicts the translation of "Machines", it needs to look at the encoder's key/value content.
                    </p>
                    <p>
                        This is called <strong>Encoder-Decoder Cross Attention</strong>.
                    </p>
                    <p>
                        The role is: <em>to let the decoder locate the correct information based on the overall meaning of the source sentence when generating each word.</em>
                    </p>
                    <p>
                        For example, when translating "bank", is it "Èì∂Ë°å" (bank) or "Ê≤≥Â≤∏" (riverbank)? We must combine the complete context of the input, which is the capability of Cross-Attention.
                    </p>

                    <h4>Function 3: Converting Semantic Information into Specific Words (Feed-Forward + Linear + Softmax)</h4>
                    <p>
                        When the decoder understands what it has generated through self-attention, and understands the source sentence through cross-attention, it combines this information to form a high-dimensional vector.
                    </p>
                    <p>
                        Then:
                    </p>
                    <ol>
                        <li>Enter FFN (two-layer feedforward network)</li>
                        <li>Go through linear layer + Softmax</li>
                        <li>Get the probability distribution of the next word</li>
                    </ol>
                    <p>
                        Finally, select the most likely next word, which is the generated result.
                    </p>

                    <h2>Summary</h2>
                    <div style="background-color: #fef3c7; border-left: 4px solid #f59e0b; padding: 1rem; margin: 1.5rem 0; border-radius: 4px;">
                        <p style="margin: 0; font-size: 1.1rem; font-weight: 600;">
                            <strong>‚≠ê Key Takeaways:</strong>
                        </p>
                        <ul style="margin: 0.5rem 0 0 0;">
                            <li>Transformer revolutionized NLP by using attention mechanisms instead of recurrence or convolution</li>
                            <li>Self-attention allows each word to attend to all other words in the sequence</li>
                            <li>Multi-head attention enables the model to focus on different types of information simultaneously</li>
                            <li>The encoder-decoder architecture with cross-attention enables effective sequence-to-sequence tasks</li>
                            <li>Positional encoding and residual connections are crucial for model performance</li>
                        </ul>
                    </div>

                    <h2>Learning Outcomes</h2>
                    <p>Through this reading and reflection, I have gained:</p>
                    <ul>
                        <li>‚úÖ Deep understanding of Transformer architecture and its components</li>
                        <li>‚úÖ Comprehension of self-attention mechanism and its calculation process</li>
                        <li>‚úÖ Understanding of multi-head attention and why it's powerful</li>
                        <li>‚úÖ Knowledge of encoder-decoder architecture and their roles</li>
                        <li>‚úÖ Insight into how Transformer processes sequential data without recurrence</li>
                        <li>‚úÖ Appreciation for the elegance of the "attention is all you need" design philosophy</li>
                    </ul>

                    <blockquote style="background-color: #f0f9ff; border-left: 4px solid #0ea5e9; padding: 1rem; margin: 1.5rem 0; border-radius: 4px;">
                        <strong>üí° Reflection:</strong> The Transformer architecture demonstrates that complex problems can be solved with elegant solutions. By focusing on attention mechanisms, the model achieves state-of-the-art performance across various NLP tasks, proving that sometimes "attention is all you need."
                    </blockquote>
                </div>
            </article>
        </main>
    </div>

    <footer class="footer">
        <div class="footer-content">
            <div class="footer-links">
                <a href="https://github.com/Yingurt001" target="_blank" title="GitHub"><img src="https://img.icons8.com/color/24/000000/github.png" alt="GitHub"></a>
                <a href="https://orcid.org/0009-0000-2900-9197" target="_blank" title="ORCID"><img src="https://upload.wikimedia.org/wikipedia/commons/archive/f/f7/20160723044737%21Orcid_icon.png" alt="ORCID"></a>
            </div>
            <div class="footer-copyright">¬© Ying Zhang</div>
        </div>
    </footer>
</body>
</html>

