<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Supervised Machine Learning Basics - Ying Zhang</title>
    <link rel="stylesheet" href="../styles.css">
    <!-- MathJax for rendering mathematical equations -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            }
        };
    </script>
</head>
<body>
    <!-- Navigation Bar -->
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-links">
                <a href="../index.html">
                    <svg fill="currentColor" viewBox="0 0 20 20"><path d="M10.707 2.293a1 1 0 00-1.414 0l-7 7a1 1 0 001.414 1.414L4 10.414V17a1 1 0 001 1h2a1 1 0 001-1v-2a1 1 0 011-1h2a1 1 0 011 1v2a1 1 0 001 1h2a1 1 0 001-1v-6.586l.293.293a1 1 0 001.414-1.414l-7-7z"></path></svg>
                    Home
                </a>
                <a href="../blog.html">
                    <svg fill="currentColor" viewBox="0 0 20 20"><path d="M2 5a2 2 0 012-2h7a2 2 0 012 2v4a2 2 0 01-2 2H9l-3 3v-3H4a2 2 0 01-2-2V5z"></path><path d="M15 7v2a4 4 0 01-4 4H9.828l-1.766 1.767c.28.149.599.233.938.233h2l3 3v-3h2a2 2 0 002-2V9a2 2 0 00-2-2h-1z"></path></svg>
                    Blog
                </a>
            </div>
        </div>
    </nav>

    <div class="container blog-full-width">
        <main class="content blog-post">
            <a href="../blog.html" class="back-to-blog">‚Üê Back to Blog</a>
            
            <article>
                <div class="blog-post-header">
                    <h1 class="blog-post-title">Supervised Machine Learning Basics</h1>
                    <div class="blog-post-meta">
                        <span>üìÖ January 22, 2025</span>
                        <span>üè∑Ô∏è Machine Learning, Statistics, Classification, Regression</span>
                    </div>
                </div>

                <div class="blog-post-content">
                    <h2>Target of Supervised Machine Learning</h2>
                    <p>
                        In supervised machine learning, we have a dataset that contains both target variable ($y$) and feature variables ($\mathbf{X}$). The goal is to learn a function that maps inputs to outputs based on example input-output pairs.
                    </p>
                    <p>
                        Examples include:
                    </p>
                    <ul>
                        <li><strong>Linear Regression</strong>: Predicting continuous values</li>
                        <li><strong>Binary Classification</strong>: Classifying into two classes</li>
                        <li><strong>Multi-class Classification</strong>: Classifying into multiple classes</li>
                        <li><strong>Multi-label Classification</strong>: Assigning multiple labels to each instance</li>
                    </ul>
                    <p>
                        <em>Note:</em> In statistics, we typically call $y$ the <strong>dependent variable</strong> and $\mathbf{X}$ the <strong>independent variables</strong> (or features).
                    </p>

                    <h2>Example: Binary Classification</h2>
                    <p>
                        For binary classification, supervised machine learning means we want a classifier that can reliably give us the correct labels for any input.
                    </p>

                    <h3>How to Measure Performance?</h3>
                    <div style="background-color: #f0f9ff; border-left: 4px solid #0ea5e9; padding: 1rem; margin: 1rem 0; border-radius: 4px;">
                        <strong>üí° Tip:</strong> We use a <strong>misclassification rate</strong> to measure performance.
                    </div>
                    <p>
                        The loss function for misclassification rate is:
                    </p>
                    <div style="text-align: center; margin: 1.5rem 0;">
                        $$\mathcal{L}(\theta) \triangleq \frac{1}{N}\sum_{n=1}^{N}\mathbb{I}(y_n \neq f(\boldsymbol{x}_n;\theta))$$
                    </div>
                    <p>
                        where $\mathbb{I}(\cdot)$ is the indicator function that returns 1 if the condition is true, and 0 otherwise.
                    </p>

                    <h2>Model Fitting: Empirical Risk Minimization</h2>
                    <p>
                        Model fitting is done by minimizing the loss function to find the optimal $\theta$, which is also called <strong>empirical risk minimization</strong>.
                    </p>
                    <div style="text-align: center; margin: 1.5rem 0;">
                        $$\hat{\theta} = \arg\min_{\theta} \mathcal{L}(\mathbf{\theta}) = \arg\min_\theta\frac{1}{N}\sum_{n=1}^{N} \ell \bigl(y_n, f(\mathbf{x}_n; \theta) \bigr)$$
                    </div>
                    <p>
                        This optimization process finds the parameters that minimize the average loss over the training data.
                    </p>

                    <h2>Uncertainty in Predictions</h2>
                    <h3>Why Do We Need Uncertainty?</h3>
                    <p>
                        We should avoid false confidence. There are two sources from which we get uncertainty:
                    </p>
                    <ul>
                        <li><strong>Epistemic uncertainty:</strong> Arises because of lack of knowledge (model uncertainty)</li>
                        <li><strong>Aleatoric uncertainty:</strong> Due to the randomness in data (data uncertainty)</li>
                    </ul>
                    <p>
                        Sometimes, the model cannot give us the exact answer to our prediction, and we need to use conditional probability distribution to describe the uncertainty:
                    </p>
                    <div style="text-align: center; margin: 1.5rem 0;">
                        $$p(y=c|\mathbf{x};\mathbf{\theta})=f_c(\mathbf{x};\mathbf{\theta})$$
                    </div>

                    <h3>Softmax Function</h3>
                    <p>
                        We use raw model scores (any real numbers) which are the direct output of our model. Theoretically, we need the function $f_c(\mathbf{x};\mathbf{\theta})$ to be within the range $(0,1)$. To avoid this restriction, we can let the model return unnormalized log-probabilities. We can then convert these to probabilities using the <strong>softmax function</strong>:
                    </p>
                    <div style="text-align: center; margin: 1.5rem 0;">
                        $$\mathcal{S}(\mathbf{a}) \triangleq \left[\frac{e^{a_1}}{\sum_{c'=1}^{C}e^{a_{c'}}},\cdots, \frac{e^{a_C}}{\sum_{c'=1}^{C}e^{a_{c'}}} \right]$$
                    </div>
                    <p>
                        We use exponentials because:
                    </p>
                    <ul>
                        <li>Exponentials are always positive</li>
                        <li>It is easy to normalize</li>
                        <li>Larger scores have exponentially larger influence</li>
                    </ul>
                    <p>
                        Hence the final probability becomes:
                    </p>
                    <div style="text-align: center; margin: 1.5rem 0;">
                        $$p(y=c|\mathbf{x};\theta)=\mathcal{S}(f(\mathbf{x};\theta))_c$$
                    </div>

                    <h2>Understanding Logistic Regression</h2>
                    <p>
                        In statistics, Logistic Regression is similar to linear regression, but for classification. We write it as:
                    </p>
                    <div style="text-align: center; margin: 1.5rem 0;">
                        $$f(\mathbf{x};\mathbf{w})=\mathbf{w}^T\mathbf{x}$$
                    </div>
                    <p>
                        However, for binary classification, we apply the sigmoid function to get probabilities:
                    </p>
                    <div style="text-align: center; margin: 1.5rem 0;">
                        $$p(y=1|\mathbf{x};\mathbf{w}) = \sigma(\mathbf{w}^T\mathbf{x}) = \frac{1}{1 + e^{-\mathbf{w}^T\mathbf{x}}}$$
                    </div>

                    <h2>Maximum Likelihood Estimation (MLE)</h2>
                    <p>
                        When fitting probabilistic models, we often use <strong>negative log likelihood</strong> as the loss function:
                    </p>
                    <div style="text-align: center; margin: 1.5rem 0;">
                        $$\ell(y,f(\mathbf{x};\mathbf{\theta}))=-\log p(y \mid f(\mathbf{x};\mathbf{\theta}))$$
                    </div>

                    <h3>Why Negative Log Likelihood?</h3>
                    <p>
                        A good model should assign a high probability to the true output $y$ for each corresponding input $\mathbf{x}$. This is done within the training set.
                    </p>
                    <p>
                        The average negative log probability of the training set is then given by:
                    </p>
                    <div style="text-align: center; margin: 1.5rem 0;">
                        $$\text{NLL}(\mathbf{\theta}) = -\frac{1}{N}\sum_{n=1}^{N}\log p(y_n\mid f(\mathbf{x}_n;\mathbf{\theta}))$$
                    </div>
                    <p>
                        In statistics, if we minimize this, we can compute the <strong>maximum likelihood estimate</strong> or MLE:
                    </p>
                    <div style="text-align: center; margin: 1.5rem 0;">
                        $$\hat{\theta}_{\text{MLE}}=\arg\min_\mathbf{\theta}\text{NLL}(\mathbf{\theta})$$
                    </div>

                    <h2>Implementation of MLE in Linear Regression</h2>
                    <p>
                        For linear regression, the MLE solution has a closed-form expression. Given the model:
                    </p>
                    <div style="text-align: center; margin: 1.5rem 0;">
                        $$y = \mathbf{w}^T\mathbf{x} + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2)$$
                    </div>
                    <p>
                        The MLE estimate for the weights is:
                    </p>
                    <div style="text-align: center; margin: 1.5rem 0;">
                        $$\hat{\mathbf{w}}_{\text{MLE}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$$
                    </div>
                    <p>
                        where $\mathbf{X}$ is the design matrix (each row is a data point) and $\mathbf{y}$ is the target vector.
                    </p>

                    <h2>Summary</h2>
                    <p>
                        This article introduced the fundamental concepts of supervised machine learning:
                    </p>
                    <ul>
                        <li>Understanding the target of supervised learning (regression and classification)</li>
                        <li>Measuring performance using loss functions</li>
                        <li>Model fitting through empirical risk minimization</li>
                        <li>Handling uncertainty using probability distributions</li>
                        <li>The softmax function for multi-class classification</li>
                        <li>Maximum likelihood estimation as a principled approach to model fitting</li>
                    </ul>
                    <p>
                        These concepts form the foundation for understanding more advanced machine learning techniques.
                    </p>

                    <blockquote style="background-color: #f0f9ff; border-left: 4px solid #0ea5e9; padding: 1rem; margin: 1.5rem 0; border-radius: 4px;">
                        <strong>Next Steps:</strong> In future posts, we'll explore specific algorithms like logistic regression, neural networks, and deep learning models, building upon these fundamental concepts.
                    </blockquote>
                </div>
            </article>
        </main>
    </div>

    <footer class="footer">
        <div class="footer-content">
            <div class="footer-links">
                <a href="https://github.com/Yingurt001" target="_blank" title="GitHub"><img src="https://img.icons8.com/color/24/000000/github.png" alt="GitHub"></a>
                <a href="https://orcid.org/0009-0000-2900-9197" target="_blank" title="ORCID"><img src="https://upload.wikimedia.org/wikipedia/commons/archive/f/f7/20160723044737%21Orcid_icon.png" alt="ORCID"></a>
            </div>
            <div class="footer-copyright">¬© Ying Zhang</div>
        </div>
    </footer>
</body>
</html>

